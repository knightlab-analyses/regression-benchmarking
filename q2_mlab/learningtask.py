import json
import numpy as np
import pandas as pd
import time
from abc import ABC

# CV Methods
from sklearn.model_selection import RepeatedStratifiedKFold
from calour.training import RepeatedSortedStratifiedKFold

# Metrics
from sklearn.metrics import (
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    accuracy_score,
    roc_auc_score,
    precision_recall_curve,
    auc,
    f1_score,
)

# Algorithms
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.linear_model import RidgeClassifier, Ridge
from xgboost import XGBRegressor, XGBClassifier
from sklearn.svm import LinearSVR, LinearSVC
from sklearn.svm import SVC, SVR
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor
from sklearn.ensemble import BaggingClassifier, BaggingRegressor
from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor
from sklearn.experimental import enable_hist_gradient_boosting
from sklearn.ensemble import (
    HistGradientBoostingClassifier,
    HistGradientBoostingRegressor,
)
from sklearn.mixture import BayesianGaussianMixture
from sklearn.naive_bayes import ComplementNB
from lightgbm import LGBMClassifier, LGBMRegressor
from sklearn.neural_network import MLPClassifier, MLPRegressor
from sklearn.linear_model import ElasticNet, Lasso


class LearningTask(ABC):
    algorithms = {}

    def __init__(
        self,
        table,
        metadata,
        algorithm,
        params,
        n_repeats,
        distance_matrix=None,
    ):
        self.distance_matrix = distance_matrix
        self.params = json.loads(params)
        self.X = table.transpose().matrix_data
        self.metadata = metadata
        self.y = self.metadata.to_numpy()
        self.learner = self.algorithms[algorithm]
        self.cv_idx = 0
        self.idx = 0
        self.n_repeats = n_repeats
        self.n_classes = self.metadata.nunique()
        self.table_size = self.n_repeats * self.y.shape[0]

        self.results = {}
        self.results["CV_IDX"] = np.zeros(self.table_size, dtype=int)
        self.results["SAMPLE_ID"] = np.zeros(self.table_size, dtype=object)
        self.results["Y_PRED"] = np.zeros(self.table_size, dtype=float)
        self.results["Y_TRUE"] = np.zeros(self.table_size, dtype=object)
        self.results["RUNTIME"] = np.zeros(self.table_size, dtype=float)

        # Check for sample id agreement between table and metadata
        if list(metadata.index) != list(table.ids()):
            raise ValueError(
                "Table and Metadata Sample IDs do not match in contents "
                "and/or order"
            )

    def contains_nan(self, y_pred):
        if (np.any(pd.isnull(y_pred))) or (not np.all(np.isfinite(y_pred))):
            return True
        else:
            return False

    def tabularize(self):
        results_table = pd.DataFrame.from_dict(self.results)
        return results_table


class ClassificationTask(LearningTask):

    algorithms = {
        "KNeighborsClassifier": KNeighborsClassifier,
        "RidgeClassifier": RidgeClassifier,
        "RandomForestClassifier": RandomForestClassifier,
        "GradientBoostingClassifier": GradientBoostingClassifier,
        "XGBClassifier": XGBClassifier,
        "RidgeClassifier": RidgeClassifier,
        "LinearSVC": LinearSVC,
        "AdaBoostClassifier": AdaBoostClassifier,
        "BaggingClassifier": BaggingClassifier,
        "ExtraTreesClassifier": ExtraTreesClassifier,
        "HistGradientBoostingClassifier": HistGradientBoostingClassifier,
        "LGBMClassifier": LGBMClassifier,
        "BayesianGaussianMixture": BayesianGaussianMixture,
        "ComplementNB": ComplementNB,
        "BayesianGaussianMixture": BayesianGaussianMixture,
        "MLPClassifier": MLPClassifier,
        "SVC": SVC,
    }

    def __init__(
        self,
        table,
        metadata,
        algorithm,
        params,
        n_repeats,
        distance_matrix=None,
    ):
        super().__init__(
            table, metadata, algorithm, params, n_repeats, distance_matrix
        )

        kfold = RepeatedStratifiedKFold(
            n_splits=5, n_repeats=self.n_repeats, random_state=2020
        )
        self.splits = kfold.split(X=self.X, y=self.y)

        for n in list(range(self.n_classes)):
            colname = "PROB_CLASS_" + str(n)
            self.results[colname] = np.zeros(self.table_size, dtype=float)
        self.results["ACCURACY"] = np.zeros(self.table_size, dtype=float)
        self.results["AUPRC"] = np.zeros(self.table_size, dtype=float)
        self.results["AUROC"] = np.zeros(self.table_size, dtype=float)
        self.results["F1"] = np.zeros(self.table_size, dtype=float)

    def cv_fold(self, train_index, test_index):
        X_train, X_test = self.X[train_index], self.X[test_index]
        y_train, y_test = self.y[train_index], self.y[test_index]
        y_test_ids = self.metadata.index[test_index]

        # Start timing
        start = time.process_time()
        m = self.learner(**self.params)
        m.fit(X_train, y_train)
        y_pred = m.predict(X_test)
        # End timimg
        end = time.process_time()

        runtime = end - start
        nrows = len(y_pred)
        curr_indices = list(range(self.idx, self.idx + nrows))
        use_probabilities = hasattr(self.learner, "predict_proba")

        if use_probabilities:
            probas = m.predict_proba(X_test)
        else:
            probas = np.nan

        self.results["RUNTIME"][curr_indices] = runtime
        self.results["CV_IDX"][curr_indices] = self.cv_idx
        self.results["Y_PRED"][curr_indices] = y_pred
        self.results["Y_TRUE"][curr_indices] = y_test
        self.results["SAMPLE_ID"][curr_indices] = y_test_ids

        for n in list(range(self.n_classes)):
            colname = "PROB_CLASS_" + str(n)
            if use_probabilities:
                self.results[colname][curr_indices] = probas[:, n]
            else:
                self.results[colname][curr_indices] = np.nan

        if self.contains_nan(y_pred):
            # All null
            self.results["AUPRC"][curr_indices] = np.nan
            self.results["AUROC"][curr_indices] = np.nan
            self.results["ACCURACY"][curr_indices] = np.nan
            self.results["F1"][curr_indices] = np.nan
        elif not use_probabilities:
            # Just F1 and Accuracy
            acc_score = accuracy_score(y_pred, y_test)
            self.results["ACCURACY"][curr_indices] = acc_score
            f1 = f1_score(y_test, y_pred)
            self.results["F1"][curr_indices] = f1

            # Others null
            self.results["AUPRC"][curr_indices] = np.nan
            self.results["AUROC"][curr_indices] = np.nan
        elif use_probabilities:
            # All metrics.
            probas = probas[:, 1]
            precision, recall, _ = precision_recall_curve(y_test, probas)
            self.results["AUPRC"][curr_indices] = auc(recall, precision)
            self.results["AUROC"][curr_indices] = roc_auc_score(y_test, probas)
            acc_score = accuracy_score(y_pred, y_test)
            self.results["ACCURACY"][curr_indices] = acc_score
            f1 = f1_score(y_test, y_pred)
            self.results["F1"][curr_indices] = f1

        self.cv_idx += 1
        self.idx += nrows


class RegressionTask(LearningTask):

    algorithms = {
        "KNeighborsRegressor": KNeighborsRegressor,
        "RandomForestRegressor": RandomForestRegressor,
        "GradientBoostingRegressor": GradientBoostingRegressor,
        "XGBRegressor": XGBRegressor,
        "AdaBoostRegressor": AdaBoostRegressor,
        "BaggingRegressor": BaggingRegressor,
        "ExtraTreesRegressor": ExtraTreesRegressor,
        "HistGradientBoostingRegressor": HistGradientBoostingRegressor,
        "LGBMRegressor": LGBMRegressor,
        "LinearSVR": LinearSVR,
        "RidgeRegressor": Ridge,
        "MLPRegressor": MLPRegressor,
        "SVR": SVR,
        "ElasticNet": ElasticNet,
        "Lasso": Lasso,
    }

    def __init__(
        self,
        table,
        metadata,
        algorithm,
        params,
        n_repeats,
        distance_matrix=None,
    ):
        super().__init__(
            table, metadata, algorithm, params, n_repeats, distance_matrix
        )

        kfold = RepeatedSortedStratifiedKFold(
            n_splits=5, n_repeats=self.n_repeats, random_state=2020
        )
        self.splits = kfold.split(X=self.X, y=self.y)

        self.results["MAE"] = np.zeros(self.table_size, dtype=float)
        self.results["RMSE"] = np.zeros(self.table_size, dtype=float)
        self.results["R2"] = np.zeros(self.table_size, dtype=float)

    def cv_fold(self, train_index, test_index):
        X_train, X_test = self.X[train_index], self.X[test_index]
        y_train, y_test = self.y[train_index], self.y[test_index]
        y_test_ids = self.metadata.index[test_index]

        # Start timing
        start = time.process_time()
        m = self.learner(**self.params)
        m.fit(X_train, y_train)
        y_pred = m.predict(X_test)
        # End timimg
        end = time.process_time()

        runtime = end - start
        nrows = len(y_pred)
        curr_indices = list(range(self.idx, self.idx + nrows))

        self.results["RUNTIME"][curr_indices] = runtime
        self.results["CV_IDX"][curr_indices] = self.cv_idx
        self.results["Y_PRED"][curr_indices] = y_pred
        self.results["Y_TRUE"][curr_indices] = y_test_ids
        self.results["SAMPLE_ID"][curr_indices] = y_test_ids

        if not self.contains_nan(y_pred):
            rmse = np.sqrt(mean_squared_error(y_test, y_pred))
            self.results["RMSE"][curr_indices] = rmse
            r2 = r2_score(y_test, y_pred)
            self.results["R2"][curr_indices] = r2
            mae = mean_absolute_error(y_test, y_pred)
            self.results["MAE"][curr_indices] = mae
        else:
            self.results["RMSE"][curr_indices] = np.nan
            self.results["R2"][curr_indices] = np.nan

        self.cv_idx += 1
        self.idx += nrows
